{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import permutations, repeat\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modelos bag of words\n",
    "2. Pre-processamento com lowercasing, stemming e remoção de caracteres não alfa-numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_text(s):\n",
    "    \n",
    "    if isinstance(s, str):\n",
    "        s = s.lower()\n",
    "        s = s.replace(\".\", \". \")\n",
    "        for s1 in range(0, 10):\n",
    "            s = s.replace(\". \" + str(s1), \".\" + str(s1))\n",
    "        \n",
    "    \n",
    "    s = s.lower()\n",
    "    s = s.replace(\" in.\",\"in.\")\n",
    "    s = s.replace(\" inch\",\"in.\")\n",
    "    s = s.replace(\"inch\",\"in.\")\n",
    "    s = s.replace(\" in \",\"in. \")\n",
    "    s = s.replace(\" ft \",\"ft. \")\n",
    "    s = s.replace(\" ft.\",\"ft.\")\n",
    "    s = s.replace(\" foot\",\"ft.\")\n",
    "    s = s.replace(\" feet\",\"ft.\")\n",
    "    s = s.replace(\"foot\",\"ft.\")\n",
    "    s = s.replace(\"feet\",\"ft.\")\n",
    "    s = s.replace(\" ft \",\"ft. \")\n",
    "    s = s.replace(\" gallon \",\"gal. \")\n",
    "    s = s.replace(\"gallon\",\"gal.\")\n",
    "    s = s.replace(\" oz.\",\"oz.\")\n",
    "    s = s.replace(\" ounce\",\"oz.\")\n",
    "    s = s.replace(\"ounce\",\"oz.\")\n",
    "    s = s.replace(\" oz \",\"oz. \")\n",
    "    s = s.replace(\" cm.\",\"cm.\")\n",
    "    s = s.replace(\" cm \",\"cm. \")\n",
    "    s = s.replace('H x', 'height')\n",
    "    s = s.replace('sq.', 'square')\n",
    "    s = s.replace('cu.', 'cubic')\n",
    "    s = s.replace('lbs.', 'pounds')\n",
    "    s = s.replace('W x', 'width')\n",
    "    s = s.replace('H x', 'height')\n",
    "    s = s.replace('Ah ', 'amphere')\n",
    "    \n",
    "    return s\n",
    "\n",
    "def clean_text(text):\n",
    "    text = correct_text(text)\n",
    "    # Remove '&nbsp;' from the text content before HTML tags strip off.\n",
    "    text.replace('&nbsp;', ' ')\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text(separator=\" \")\n",
    "    # Replace all punctuation and special characters by space\n",
    "    text.replace(\"[ &<>)(_,.;:!?/-]+\", \" \")\n",
    "    # Remove the apostrophe's\n",
    "    text.replace(\"'s\\\\b\", \"\")\n",
    "    # Remove the apostrophe\n",
    "    text.replace(\"[']+\", \"\")\n",
    "    # Remove the double quotes\n",
    "    text.replace(\"[\\\"]+\", \"\")\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = text.lower().split()\n",
    "    return( \" \".join( words ))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train.csv', encoding=\"ISO-8859-1\")\n",
    "test_data = pd.read_csv('../data/test.csv', encoding=\"ISO-8859-1\")\n",
    "attribute_data = pd.read_csv('../data/attributes.csv')\n",
    "descriptions = pd.read_csv('../data/product_descriptions.csv')\n",
    "\n",
    "train_data = pd.merge(train_data, descriptions, on=\"product_uid\", how=\"left\")\n",
    "test_data = pd.merge(test_data, descriptions, on=\"product_uid\", how=\"left\")\n",
    "\n",
    "product_count = pd.DataFrame(pd.Series(train_data.groupby([\"product_uid\"]).size(), name=\"product_count\"))\n",
    "product_count = pd.DataFrame(pd.Series(test_data.groupby([\"product_uid\"]).size(), name=\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    tokens = [token for token in tokens if token not in english_sw]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sw = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['product_description'] = train_data.apply(lambda x: clean_text(x['product_description']),axis=1)\n",
    "train_data['product_title'] = train_data.apply(lambda x: clean_text(x['product_title']),axis=1)\n",
    "train_data['search_term'] = train_data.apply(lambda x: clean_text(x['search_term']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['product_description'] = test_data.apply(lambda x: clean_text(x['product_description']),axis=1)\n",
    "test_data['product_title'] = test_data.apply(lambda x: clean_text(x['product_title']),axis=1)\n",
    "test_data['search_term'] = test_data.apply(lambda x: clean_text(x['search_term']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['description_tokens'] = train_data.apply(lambda x: word_tokenize(x['product_description']), axis=1)\n",
    "train_data['description_tokens_clean'] = train_data.apply(lambda x: clean(x['product_description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['description_tokens'] = test_data.apply(lambda x: word_tokenize(x['product_description']), axis=1)\n",
    "test_data['description_tokens_clean'] = test_data.apply(lambda x: clean(x['product_description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['title_tokens'] = train_data.apply(lambda x: word_tokenize(x['product_title']), axis=1)\n",
    "train_data['title_tokens_clean'] = train_data.apply(lambda x: clean(x['product_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['title_tokens'] = test_data.apply(lambda x: word_tokenize(x['product_title']), axis=1)\n",
    "test_data['title_tokens_clean'] = test_data.apply(lambda x: clean(x['product_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['search_tokens'] = train_data.apply(lambda x: word_tokenize(x['search_term']), axis=1)\n",
    "train_data['search_tokens_clean'] = train_data.apply(lambda x: clean(x['search_term']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['search_tokens'] = test_data.apply(lambda x: word_tokenize(x['search_term']), axis=1)\n",
    "test_data['search_tokens_clean'] = test_data.apply(lambda x: clean(x['search_term']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['n_tokens_desc'] = train_data.apply(lambda x: len(x['description_tokens']), axis=1)\n",
    "train_data['n_tokens_title'] = train_data.apply(lambda x: len(x['title_tokens']), axis=1)\n",
    "train_data['n_tokens_search'] = train_data.apply(lambda x: len(x['search_tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['n_tokens_desc'] = test_data.apply(lambda x: len(x['description_tokens']), axis=1)\n",
    "test_data['n_tokens_title'] = test_data.apply(lambda x: len(x['title_tokens']), axis=1)\n",
    "test_data['n_tokens_search'] = test_data.apply(lambda x: len(x['search_tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>description_tokens</th>\n",
       "      <th>description_tokens_clean</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tokens_clean</th>\n",
       "      <th>search_tokens</th>\n",
       "      <th>search_tokens_clean</th>\n",
       "      <th>n_tokens_desc</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strong-tie 12-gauge angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.0</td>\n",
       "      <td>not only do angles make joints stronger, they ...</td>\n",
       "      <td>[not, only, do, angles, make, joints, stronger...</td>\n",
       "      <td>[angl, make, joint, stronger, also, provid, co...</td>\n",
       "      <td>[simpson, strong-tie, 12-gauge, angle]</td>\n",
       "      <td>[simpson, strong, tie, 12, gaug, angl]</td>\n",
       "      <td>[angle, bracket]</td>\n",
       "      <td>[angl, bracket]</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strong-tie 12-gauge angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.5</td>\n",
       "      <td>not only do angles make joints stronger, they ...</td>\n",
       "      <td>[not, only, do, angles, make, joints, stronger...</td>\n",
       "      <td>[angl, make, joint, stronger, also, provid, co...</td>\n",
       "      <td>[simpson, strong-tie, 12-gauge, angle]</td>\n",
       "      <td>[simpson, strong, tie, 12, gaug, angl]</td>\n",
       "      <td>[l, bracket]</td>\n",
       "      <td>[l, bracket]</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>behr premium textured deckover 1-gal. #sc-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.0</td>\n",
       "      <td>behr premium textured deckover is an innovativ...</td>\n",
       "      <td>[behr, premium, textured, deckover, is, an, in...</td>\n",
       "      <td>[behr, premium, textur, deckov, innov, solid, ...</td>\n",
       "      <td>[behr, premium, textured, deckover, 1-gal, ., ...</td>\n",
       "      <td>[behr, premium, textur, deckov, 1, gal, sc, 14...</td>\n",
       "      <td>[deck, over]</td>\n",
       "      <td>[deck]</td>\n",
       "      <td>196</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  simpson strong-tie 12-gauge angle   \n",
       "1   3       100001                  simpson strong-tie 12-gauge angle   \n",
       "2   9       100002  behr premium textured deckover 1-gal. #sc-141 ...   \n",
       "\n",
       "     search_term  relevance  \\\n",
       "0  angle bracket        3.0   \n",
       "1      l bracket        2.5   \n",
       "2      deck over        3.0   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  not only do angles make joints stronger, they ...   \n",
       "1  not only do angles make joints stronger, they ...   \n",
       "2  behr premium textured deckover is an innovativ...   \n",
       "\n",
       "                                  description_tokens  \\\n",
       "0  [not, only, do, angles, make, joints, stronger...   \n",
       "1  [not, only, do, angles, make, joints, stronger...   \n",
       "2  [behr, premium, textured, deckover, is, an, in...   \n",
       "\n",
       "                            description_tokens_clean  \\\n",
       "0  [angl, make, joint, stronger, also, provid, co...   \n",
       "1  [angl, make, joint, stronger, also, provid, co...   \n",
       "2  [behr, premium, textur, deckov, innov, solid, ...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0             [simpson, strong-tie, 12-gauge, angle]   \n",
       "1             [simpson, strong-tie, 12-gauge, angle]   \n",
       "2  [behr, premium, textured, deckover, 1-gal, ., ...   \n",
       "\n",
       "                                  title_tokens_clean     search_tokens  \\\n",
       "0             [simpson, strong, tie, 12, gaug, angl]  [angle, bracket]   \n",
       "1             [simpson, strong, tie, 12, gaug, angl]      [l, bracket]   \n",
       "2  [behr, premium, textur, deckov, 1, gal, sc, 14...      [deck, over]   \n",
       "\n",
       "  search_tokens_clean  n_tokens_desc  n_tokens_title  n_tokens_search  \n",
       "0     [angl, bracket]            147               4                2  \n",
       "1        [l, bracket]            147               4                2  \n",
       "2              [deck]            196              13                2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bag-of-Word Features\n",
    "\n",
    "1. feature_1 = is search term a substring in title?\n",
    "2. feature_2 = is search term a substring in description?\n",
    "3. feature_3 = proportion of terms of search in title - no stemm, no stopword removal\n",
    "4. feature_4 = proportion of terms of search in title - stemm and stopword removal\n",
    "5. feature_5 = proportion of terms of search in description - no stemm, no stopword removal\n",
    "6. feature_6 = proportion of terms of search in description - stemm and stopword removal\n",
    "7. feature_7 = length of search\n",
    "8. feature_8 = length of description\n",
    "9. feature_9 = length of title\n",
    "7. features_10 to 43 = if word i in search is in description\n",
    "7. features_44 to 77 = if word i in search is in title\n",
    "\n",
    "#### Ideias\n",
    "\n",
    "1. 1 feature para cada palavra de busca indicando se ela está no titulo ou descrição\n",
    "2. Usando word2vec, distance entre palavras da busca e do titulo/descrição\n",
    "3. Usando topic modeling: Probabilidade de busca conter o mesmo topico que titulo/descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['feature_1'] = train_data.apply(lambda x: int(x['search_term'] in x['product_title']), axis=1)\n",
    "train_data['feature_2'] = train_data.apply(lambda x: int(x['search_term'] in x['product_description']), axis=1)\n",
    "train_data['feature_3'] = train_data.apply(lambda x: float(len( set(x['search_tokens']).intersection(set(x['title_tokens']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "train_data['feature_4'] = train_data.apply(lambda x: float(len( set(x['search_tokens_clean']).intersection(set(x['title_tokens_clean']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "train_data['feature_5'] = train_data.apply(lambda x: float(len( set(x['search_tokens']).intersection(set(x['description_tokens']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "train_data['feature_6'] = train_data.apply(lambda x: float(len( set(x['search_tokens_clean']).intersection(set(x['description_tokens_clean']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "train_data['feature_7'] = train_data.apply(lambda x: float(len(x['search_tokens'])), axis=1)\n",
    "train_data['feature_8'] = train_data.apply(lambda x: float(len(x['description_tokens'])), axis=1)\n",
    "train_data['feature_9'] = train_data.apply(lambda x: float(len(x['title_tokens'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['feature_1'] = test_data.apply(lambda x: int(x['search_term'] in x['product_title']), axis=1)\n",
    "test_data['feature_2'] = test_data.apply(lambda x: int(x['search_term'] in x['product_description']), axis=1)\n",
    "test_data['feature_3'] = test_data.apply(lambda x: float(len( set(x['search_tokens']).intersection(set(x['title_tokens']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "test_data['feature_4'] = test_data.apply(lambda x: float(len( set(x['search_tokens_clean']).intersection(set(x['title_tokens_clean']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "test_data['feature_5'] = test_data.apply(lambda x: float(len( set(x['search_tokens']).intersection(set(x['description_tokens']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "test_data['feature_6'] = test_data.apply(lambda x: float(len( set(x['search_tokens_clean']).intersection(set(x['description_tokens_clean']))))/len( set(x['search_tokens'])) , axis=1)\n",
    "test_data['feature_7'] = test_data.apply(lambda x: float(len(x['search_tokens'])), axis=1)\n",
    "test_data['feature_8'] = test_data.apply(lambda x: float(len(x['description_tokens'])), axis=1)\n",
    "test_data['feature_9'] = test_data.apply(lambda x: float(len(x['title_tokens'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>description_tokens</th>\n",
       "      <th>description_tokens_clean</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tokens_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>n_tokens_search</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strong-tie 12-gauge angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.0</td>\n",
       "      <td>not only do angles make joints stronger, they ...</td>\n",
       "      <td>[not, only, do, angles, make, joints, stronger...</td>\n",
       "      <td>[angl, make, joint, stronger, also, provid, co...</td>\n",
       "      <td>[simpson, strong-tie, 12-gauge, angle]</td>\n",
       "      <td>[simpson, strong, tie, 12, gaug, angl]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strong-tie 12-gauge angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.5</td>\n",
       "      <td>not only do angles make joints stronger, they ...</td>\n",
       "      <td>[not, only, do, angles, make, joints, stronger...</td>\n",
       "      <td>[angl, make, joint, stronger, also, provid, co...</td>\n",
       "      <td>[simpson, strong-tie, 12-gauge, angle]</td>\n",
       "      <td>[simpson, strong, tie, 12, gaug, angl]</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>behr premium textured deckover 1-gal. #sc-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.0</td>\n",
       "      <td>behr premium textured deckover is an innovativ...</td>\n",
       "      <td>[behr, premium, textured, deckover, is, an, in...</td>\n",
       "      <td>[behr, premium, textur, deckov, innov, solid, ...</td>\n",
       "      <td>[behr, premium, textured, deckover, 1-gal, ., ...</td>\n",
       "      <td>[behr, premium, textur, deckov, 1, gal, sc, 14...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>196</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  simpson strong-tie 12-gauge angle   \n",
       "1   3       100001                  simpson strong-tie 12-gauge angle   \n",
       "2   9       100002  behr premium textured deckover 1-gal. #sc-141 ...   \n",
       "\n",
       "     search_term  relevance  \\\n",
       "0  angle bracket        3.0   \n",
       "1      l bracket        2.5   \n",
       "2      deck over        3.0   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  not only do angles make joints stronger, they ...   \n",
       "1  not only do angles make joints stronger, they ...   \n",
       "2  behr premium textured deckover is an innovativ...   \n",
       "\n",
       "                                  description_tokens  \\\n",
       "0  [not, only, do, angles, make, joints, stronger...   \n",
       "1  [not, only, do, angles, make, joints, stronger...   \n",
       "2  [behr, premium, textured, deckover, is, an, in...   \n",
       "\n",
       "                            description_tokens_clean  \\\n",
       "0  [angl, make, joint, stronger, also, provid, co...   \n",
       "1  [angl, make, joint, stronger, also, provid, co...   \n",
       "2  [behr, premium, textur, deckov, innov, solid, ...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0             [simpson, strong-tie, 12-gauge, angle]   \n",
       "1             [simpson, strong-tie, 12-gauge, angle]   \n",
       "2  [behr, premium, textured, deckover, 1-gal, ., ...   \n",
       "\n",
       "                                  title_tokens_clean    ...      \\\n",
       "0             [simpson, strong, tie, 12, gaug, angl]    ...       \n",
       "1             [simpson, strong, tie, 12, gaug, angl]    ...       \n",
       "2  [behr, premium, textur, deckov, 1, gal, sc, 14...    ...       \n",
       "\n",
       "  n_tokens_search feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0               2         0          0        0.5        0.5        0.0   \n",
       "1               2         0          0        0.0        0.0        0.0   \n",
       "2               2         0          0        0.0        0.0        0.5   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  \n",
       "0        0.5          2        147          4  \n",
       "1        0.0          2        147          4  \n",
       "2        0.5          2        196         13  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in test_data['search_tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_search(word_list, index, tokens):\n",
    "    if len(word_list) < index + 1:\n",
    "        return 0\n",
    "    text = ''.join(tokens)\n",
    "    return int(word_list[index] in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in range(0,14):\n",
    "    train_data['feature_{}'.format(index+10)] = train_data.apply(lambda x: word_search(x['search_tokens'], index, x['title_tokens']), axis=1)\n",
    "    test_data['feature_{}'.format(index+10)] = test_data.apply(lambda x: word_search(x['search_tokens'], index, x['title_tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in range(0,14):\n",
    "    train_data['feature_{}'.format(index+24)] = train_data.apply(lambda x: word_search(x['search_tokens'], index, x['description_tokens']), axis=1)\n",
    "    test_data['feature_{}'.format(index+24)] = test_data.apply(lambda x: word_search(x['search_tokens'], index, x['description_tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                      u'id',              u'product_uid',\n",
       "                  u'product_title',              u'search_term',\n",
       "                      u'relevance',      u'product_description',\n",
       "             u'description_tokens', u'description_tokens_clean',\n",
       "                   u'title_tokens',       u'title_tokens_clean',\n",
       "                  u'search_tokens',      u'search_tokens_clean',\n",
       "                  u'n_tokens_desc',           u'n_tokens_title',\n",
       "                u'n_tokens_search',                u'feature_1',\n",
       "                      u'feature_2',                u'feature_3',\n",
       "                      u'feature_4',                u'feature_5',\n",
       "                      u'feature_6',                u'feature_7',\n",
       "                      u'feature_8',                u'feature_9',\n",
       "                     u'feature_10',               u'feature_11',\n",
       "                     u'feature_12',               u'feature_13',\n",
       "                     u'feature_14',               u'feature_15',\n",
       "                     u'feature_16',               u'feature_17',\n",
       "                     u'feature_18',               u'feature_19',\n",
       "                     u'feature_20',               u'feature_21',\n",
       "                     u'feature_22',               u'feature_23',\n",
       "                     u'feature_24',               u'feature_25',\n",
       "                     u'feature_26',               u'feature_27',\n",
       "                     u'feature_28',               u'feature_29',\n",
       "                     u'feature_30',               u'feature_31',\n",
       "                     u'feature_32',               u'feature_33',\n",
       "                     u'feature_34',               u'feature_35',\n",
       "                     u'feature_36',               u'feature_37'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_features_train = train_data[['id', 'relevance'] + ['feature_{0}'.format(i) for i in range(1,38)]]\n",
    "bow_features_test = test_data[['id'] + ['feature_{0}'.format(i) for i in range(1,38)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_features_train.to_csv('../data/bow_features.csv', index=False)\n",
    "bow_features_test.to_csv('../data/bow_features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Features\n",
    "\n",
    "1. feature_1 = is search term a substring in title?\n",
    "2. feature_2 = is search term a substring in description?\n",
    "3. feature_3 = proportion of terms of search in title - no stemm, no stopword removal\n",
    "4. feature_4 = proportion of terms of search in title - stemm and stopword removal\n",
    "5. feature_5 = proportion of terms of search in description - no stemm, no stopword removal\n",
    "6. feature_6 = proportion of terms of search in description - stemm and stopword removal\n",
    "7. feature_7 = length of search\n",
    "8. feature_8 = length of description\n",
    "9. feature_9 = length of title\n",
    "7. features_10 to 23 = lowest similarity of word i in search and the words in the description\n",
    "7. features_24 to 37 = lowest similarity of word i in search and the words in the title\n",
    "\n",
    "#### Ideias\n",
    "\n",
    "1. 1 feature para cada palavra de busca indicando se ela está no titulo ou descrição\n",
    "2. Usando word2vec, distance entre palavras da busca e do titulo/descrição\n",
    "3. Usando topic modeling: Probabilidade de busca conter o mesmo topico que titulo/descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptions = list(train_data.product_description)\n",
    "title = list(train_data.product_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = descriptions + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [[word for word in sentence.lower().split()] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "    for token in sentence:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "sentences = [[token for token in sentence if frequency[token] > 1] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#query_terms = set(itertools.chain(*train_data.search_tokens_clean))\n",
    "#title_terms = set(itertools.chain(*train_data.title_tokens_clean))\n",
    "#description_terms = set(itertools.chain(*train_data.description_tokens_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowest_similarity(search_token_list, attribute_token_list, index, model):\n",
    "    \n",
    "    if len(search_token_list) < index + 1:\n",
    "        return 0.\n",
    "    else:\n",
    "        search_token = search_token_list[index]\n",
    "    \n",
    "    similarities = []\n",
    "    tuples = [[search_token, attribute_token]  for attribute_token in attribute_token_list]\n",
    "    \n",
    "    for x,y in tuples:\n",
    "        try:\n",
    "            similarities.append(model.similarity(x,y))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if not similarities:\n",
    "        return 0.\n",
    "    \n",
    "    return min(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in range(0,17):\n",
    "    train_data['feature_{}'.format(index+10)] = train_data.apply(lambda x: lowest_similarity(x['search_tokens'], x['title_tokens'], index, model), axis=1)\n",
    "    test_data['feature_{}'.format(index+10)] = test_data.apply(lambda x: lowest_similarity(x['search_tokens'], x['title_tokens'], index, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in range(0,17):\n",
    "    train_data['feature_{}'.format(index+27)] = train_data.apply(lambda x: lowest_similarity(x['search_tokens'], x['description_tokens'], index, model), axis=1)\n",
    "    test_data['feature_{}'.format(index+27)] = test_data.apply(lambda x: lowest_similarity(x['search_tokens'], x['description_tokens'], index, model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_features_train = train_data[['id', 'relevance'] + ['feature_{0}'.format(i) for i in range(1,38)]]\n",
    "w2v_features_test = test_data[['id'] + ['feature_{0}'.format(i) for i in range(1,38)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_features_train.to_csv('../data/w2v_features.csv', index=False)\n",
    "w2v_features_test.to_csv('../data/w2v_features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes.csv\t\t  features.csv\t\t       test.csv\r\n",
      "bag_of_word_features.csv  product_descriptions.csv     train.csv\r\n",
      "bow_features.csv\t  relevance_instructions.docx  w2v_features.csv\r\n",
      "bow_features_test.csv\t  sample_submission.csv        w2v_features_test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
